import argparse
import csv
import logging
import os
import re
import zipfile
from typing import List

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoTokenizer,
)

# Bind to GPU 1 by default
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "1")


SAFE_REMOVE_TOKENS = [
    # Ñ€ÑƒÑÑÐºÐ¸Ðµ Ð¼Ð°Ñ‚Ð½Ñ‹Ðµ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸, Ñ‡Ð°ÑÑ‚Ð¾ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÑŽÑ‰Ð¸ÐµÑÑ Ð² Ñ‚Ð°Ñ‚Ð°Ñ€ÑÐºÐ¾Ð¹ Ñ€ÐµÑ‡Ð¸
    "Ð±Ð»Ñ", "Ð±Ð»ÑÑ", "Ð±Ð»ÑÑ‚", "Ð±Ð»ÑÑ‚ÑŒ", "Ð±Ð»ÑÐ´ÑŒ", "Ð±Ð»ÑÑ‚", "Ð±Ð»Ñ", "Ð±Ð»Ñ‚",
    "Ð½Ð°Ñ…", "Ð½Ð°Ñ…ÑƒÐ¹", "Ð½Ð°Ñ…Ñ€ÐµÐ½", "Ð½Ð°Ñ…ÐµÑ€", "Ð½Ð°Ñ…ÑƒÑ",

    # Ð¿Ð¸Ð·Ð´-ÐºÐ¾Ñ€ÐµÐ½ÑŒ (ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÐ¶Ð´Ð¾Ð¼ÐµÑ‚Ð¸Ñ, Ð±ÐµÐ· ÑÐ¼Ñ‹ÑÐ»Ð°)
    "Ð¿Ð¸Ð·Ð´Ð°", "Ð¿ÐµÐ·Ð´Ð°", "Ð¿ÐµÐ·Ð´Ð¾Ñ", "Ð¿Ð¸Ð·Ð´Ð¾Ñ", "Ð¿Ð¸Ð·Ð´ÑŽÐº", "Ð¿ÐµÐ·Ð´ÑŽÐº",
    "Ð¿Ð¸Ð·Ð´ÐµÑ†", "Ð¿ÐµÐ·Ð´ÑÑ†", "Ð¿ÐµÐ·Ð´ÐµÑ", "Ð¿Ð¸Ð·Ð´Ð¾ÑÐ¸Ðº", "Ð¿Ð¸Ð·Ð´Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ",
    "Ð¿Ð¸Ð·Ð´Ð¾Ð±Ð¾Ð»", "Ð¿Ð¸Ð·Ð´Ð°Ð±Ð¾Ð»",

    # Ñ…ÑƒÐµ-ÐºÐ¾Ñ€ÐµÐ½ÑŒ
    "Ñ…ÑƒÐµÐ¿Ð»ÐµÑ‚", "Ñ…ÑƒÐµÐ¿Ð»Ñ‘Ñ‚", "Ñ…ÑƒÐµÑÐ¾Ñ", "Ñ…ÑƒÐµÑÐ¾ÑÐ»Ð°Ñ€", "Ñ…ÑƒÐµÐ¿Ð°Ñ‡Ð¼Ð°Ðº",
    "Ñ…ÑƒÐµÐ±ÐµÑ‚", "Ñ…ÑƒÐµÐ±Ð¾Ð·", "Ñ…ÑƒÐ¹Ð»Ð¾Ð¿", "Ñ…ÑƒÐ¹Ð»Ð¾", "Ñ…ÑƒÐ¹Ð½Ñ",
    "Ñ…ÑƒÐ¸Ñ‚Ð°", "Ñ…ÑƒÑ‘Ð²", "Ñ…ÑƒÐµÐ²Ð¸Ð½Ð°",

    # Ð¾ÑÐºÐ¾Ñ€Ð±Ð»ÐµÐ½Ð¸Ñ, Ð½Ðµ Ð½ÐµÑÑƒÑ‰Ð¸Ðµ Ñ„Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¼Ñ‹ÑÐ»Ð°
    "Ð¿Ð¸Ð´Ð¾Ñ€", "Ð¿Ð¸Ð´Ð°Ñ€Ð°Ñ", "Ð¿Ð¸Ð´Ð¾Ñ€Ð°Ñ", "Ð¿Ð¸Ð´Ð°Ñ€", "Ð¿Ð¸Ð´Ñ€Ð¸Ð»Ð»Ð°",
    "Ð¿Ð¸Ð´Ð°Ñ€Ð¾Ðº", "Ð¿Ð¸Ð´Ñ€", "Ð¿Ð¸Ð´Ñ€Ð¸Ñ", "Ð¿ÐµÐ´Ð¸Ðº", "Ð³Ð¾Ð¼Ð¸Ðº",

    # Ð¼ÑÐ³ÐºÐ°Ñ Ñ€ÑƒÐ³Ð°Ð½ÑŒ
    "Ð´Ð¾Ð»Ð±Ð°ÐµÐ±", "Ð´Ð¾Ð»Ð±Ð¾ÐµÐ±", "Ð´Ð¾Ð»Ð±Ð¾Ñ‘Ð±", "Ð´ÐµÐ±Ð¸Ð»", "Ð¸Ð´Ð¸Ð¾Ñ‚",
    "Ñ‡Ð¼Ð¾", "Ñ‡Ð¼Ð¾ÑˆÐ½Ð¸Ðº",

    # ÑÐºÐ°Ñ‚Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÑƒÐ³Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°
    "ÑÑ€Ð°Ð½Ð´ÐµÐ»ÑŒ", "ÑÑ€Ð°Ñ‚Ñ‹Ð¹", "Ð³Ð¾Ð²Ð½ÑŽÐº", "Ð³Ð°Ð²Ð½ÑŽÐº",

    # Â«Ð¶Ð¾Ð¿Ð°Â»-Ñ€ÑƒÐ³Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°
    "Ð¶Ð¾Ð¿Ð°", "Ð¶Ð¾Ð¿Ð¾Ñ€Ð¾Ñ‚Ñ‹Ð¹", "Ð¶Ð¾Ð¿Ð¾Ð»Ð¸Ð·Ð°", "Ð¶Ð¾Ð¿Ð°ÑˆÐ½Ð¸Ðº",

    # Ñ‚Ð°Ñ‚Ð°Ñ€ÑÐºÐ¸Ðµ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÐ¶Ð´Ð¾Ð¼ÐµÑ‚Ð¸Ñ
    "Ó™Ñ‚Ñ‚Ó™Ð³ÐµÐ½Ó™", "Ó™Ñ‚Ñ‚Ó™Ð³ÐµÐ½Ó™Ò»Ðµ", "Ó™Ð¿Ó™Ñ‚", "Ó™Ð¿Ó™Ó™Ñ‚",

    # Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ€ÑƒÐ³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚Ñ‹
    "ÑÑƒÐºÐ°", "ÑÑƒÑ‡Ð°Ñ€Ð°", "Ð¼Ñ€Ð°Ð·ÑŒ", "Ñ‚Ð²Ð°Ñ€ÑŒ",

    # Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ñ‹Ðµ ÑÐ¼Ð¾Ð´Ð·Ð¸ / Ð¸Ñ… ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ð¸
    "ðŸ˜¡", "ðŸ¤¬", "ðŸ‘¿", "ðŸ˜ ",
    "ðŸ˜", "ðŸ˜’", "ðŸ™„",
    "ðŸ’©", "ðŸ‘", "ðŸ‘‰ðŸ‘Œ", "ðŸ‘ˆðŸ‘‰", "ðŸ”¥ðŸ‘", "ðŸ†", "ðŸ’¦ðŸ‘", "ðŸ’¦ðŸ†",
    "ðŸ¤¡", "ðŸ–•", "ðŸ–•ðŸ»", "ðŸ–•ðŸ½", "ðŸ–•ðŸ¿", "ðŸ¤¦", "ðŸ¤¦â€â™‚ï¸", "ðŸ¤¦â€â™€ï¸", "ðŸ¤·", "ðŸ¤·â€â™‚ï¸", "ðŸ¤·â€â™€ï¸",
    "ðŸ˜‚ðŸ’©", "ðŸ’©ðŸ˜‚", "ðŸ¤¡ðŸ˜‚", "ðŸ˜‚ðŸ¤¡", "ðŸ¤¬ðŸ¤¬", "ðŸ˜¡ðŸ¤¬", "ðŸ™ƒ",
]


def get_device() -> torch.device:
    if torch.cuda.is_available():
        try:
            torch.cuda.set_device(0)
        except Exception:
            pass
        return torch.device("cuda:0")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


class ToxicityClassifier:
    def __init__(self, device: torch.device, max_len: int = 512):
        self.tok = AutoTokenizer.from_pretrained(
            "textdetox/xlmr-large-toxicity-classifier-v2"
        )
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "textdetox/xlmr-large-toxicity-classifier-v2"
        ).to(device)
        self.model.eval()
        self.device = device
        self.max_len = max_len

    @torch.no_grad()
    def score(self, texts: List[str], batch_size: int = 16) -> np.ndarray:
        probs = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            enc = self.tok(
                batch,
                padding=True,
                truncation=True,
                max_length=self.max_len,
                return_tensors="pt",
            ).to(self.device)
            logits = self.model(**enc).logits
            p = torch.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()
            probs.append(p)
        return np.concatenate(probs) if probs else np.array([])


class LlamaDetox:
    SYSTEM_PROMPT = (
        "You are an expert in text detoxification. Rewrite the input so that:\n"
        "1) Meaning, intent, and factual content are preserved.\n"
        "2) Remove insults, slurs, profanity, hate speech, degrading or passiveâ€‘aggressive tone.\n"
        "3) Keep the sentence natural and fluent, stylistically close to original.\n"
        "4) Do NOT change topic, invent facts, or distort events â€” only soften toxic phrasing.\n"
        "5) Preserve as much semantic information as possible.\n\n"
        "Reply with the rewritten sentence only."
    )

    def __init__(self, model_id: str, device: torch.device, fp16: bool = True):
        if device.type == "cuda" and fp16:
            self.model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(model_id).to(device)
        self.tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
        self.tok.padding_side = "left"
        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token
        if getattr(self.model.config, "pad_token_id", None) is None:
            self.model.config.pad_token_id = self.tok.pad_token_id
        self.device = device
        self.model.eval()

    def _build_messages(self, text: str) -> List[dict]:
        return [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {"role": "user", "content": f"Input: {text}\nOutput:"},
        ]

    @torch.no_grad()
    def detox(self, texts: List[str], batch_size: int = 8, max_new_tokens: int = 128, temperature: float = 0.7, top_p: float = 0.95, single_line: bool = True) -> List[str]:
        outs: List[str] = []
        total = (len(texts) + batch_size - 1) // batch_size
        for i in tqdm(range(0, len(texts), batch_size), total=total, desc="LLaMA detox", unit="batch"):
            batch = texts[i : i + batch_size]
            msgs = [self._build_messages(t) for t in batch]
            rendered = self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
            enc = self.tok(rendered, return_tensors="pt", padding=True, truncation=True, max_length=2048).to(self.device)
            gen = self.model.generate(
                **enc,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tok.pad_token_id,
                eos_token_id=self.tok.eos_token_id,
            )
            input_lengths = enc["attention_mask"].sum(dim=1).tolist()
            for j, in_len in enumerate(input_lengths):
                new_txt = self.tok.decode(gen[j, in_len:], skip_special_tokens=True)
                if single_line:
                    new_txt = re.sub(r"\s+", " ", new_txt).strip()
                outs.append(new_txt)
        return outs


def build_regexes() -> List[re.Pattern]:
    patterns = []
    # ÐžÐ±Ñ„ÑƒÑÐºÐ°Ñ†Ð¸Ð¸ Ð²Ð¾ÐºÑ€ÑƒÐ³ Â«Ð±Ð»ÑÑ‚ÑŒÂ»: Ð±Ð»*Ñ‚ÑŒ, Ð±Ð»##Ñ‚ÑŒ, Ð±Ð»ÐµÐ°Ñ‚ÑŒ, Ð±Ð»ÑÑ‚
    patterns.append(re.compile(r"Ð±\s*Ð»\s*[Ðµe*#]?\s*[Ñya@4]+\s*(?:Ñ‚|Ñ‚ÑŒ|\*+|#+)", re.IGNORECASE))
    # ÐžÐ±Ñ‰Ð¸Ðµ ÐºÐ¾Ñ€Ð½Ð¸ ÑÐ¾ Ð·Ð²Ñ‘Ð·Ð´Ð¾Ñ‡ÐºÐ°Ð¼Ð¸/Ñ€ÐµÑˆÑ‘Ñ‚ÐºÐ°Ð¼Ð¸ (Ñ…Ñƒ*, Ð¿Ð¸*Ð´*, Ð½Ð°*ÑƒÐ¹ â€¦)
    patterns.append(re.compile(r"Ñ…\s*Ñƒ\s*[Ð¹Ð¸i\*#]+[Ð°-ÑÑ‘]*", re.IGNORECASE))
    patterns.append(re.compile(r"Ð¿\s*Ð¸\s*Ð·\s*Ð´[Ð°-ÑÑ‘\*#]+", re.IGNORECASE))
    patterns.append(re.compile(r"Ð½\s*Ð°\s*Ñ…[Ð°-ÑÑ‘\*#]+", re.IGNORECASE))
    return patterns


REGEXES = build_regexes()


def cleanse_text(text: str) -> str:
    t = str(text)
    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÐ²Ð½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ (Ð±ÐµÐ· ÑƒÑ‡Ñ‘Ñ‚Ð° Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°)
    for tok in SAFE_REMOVE_TOKENS:
        t = re.sub(re.escape(tok), "", t, flags=re.IGNORECASE)
    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¾Ð±Ñ„ÑƒÑÐºÐ°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€ÐºÐ°Ð¼
    for rx in REGEXES:
        t = rx.sub("", t)
    # Ð¡Ñ…Ð»Ð¾Ð¿Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹
    t = re.sub(r"\s+", " ", t).strip()
    return t


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input_tsv", type=str, default="DETOX_TATAR/data/dev_inputs.tsv")
    ap.add_argument("--output_dir", type=str, default="DETOX_TATAR/outputs_wordrm_llama")
    ap.add_argument("--threshold", type=float, default=0.5)
    ap.add_argument("--batch_size", type=int, default=16)
    ap.add_argument("--fp16", type=lambda x: str(x).lower() == "true", default=True)
    ap.add_argument("--llama_id", type=str, default="meta-llama/Meta-Llama-3.1-8B-Instruct")
    ap.add_argument("--max_new_tokens", type=int, default=128)
    ap.add_argument("--temperature", type=float, default=0.7)
    ap.add_argument("--top_p", type=float, default=0.95)
    ap.add_argument("--zip", dest="make_zip", action="store_true")
    args = ap.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
    logging.info("CUDA_VISIBLE_DEVICES=%s", os.environ.get("CUDA_VISIBLE_DEVICES"))
    device = get_device()
    logging.info("Device: %s", device)

    # Load data
    df = pd.read_csv(args.input_tsv, sep="\t", quoting=csv.QUOTE_NONE)
    if not {"ID", "tat_toxic"}.issubset(df.columns):
        raise ValueError("Input TSV must contain 'ID' and 'tat_toxic'")
    ids = df["ID"].tolist()
    texts = df["tat_toxic"].astype(str).tolist()
    logging.info("Loaded %d rows", len(texts))

    # 1) Cleaning
    logging.info("Applying word removal + regex cleaning")
    cleaned = [cleanse_text(t) for t in tqdm(texts, desc="Clean", unit="row")]

    # 2) Classifier gating
    clf = ToxicityClassifier(device)
    logging.info("Scoring toxicity after cleaning (gating)")
    probs = clf.score(cleaned, batch_size=args.batch_size)
    mask = (probs >= args.threshold).tolist()

    # 3) LLaMA detox only for masked items; keep new tokens only, single line
    llama = LlamaDetox(args.llama_id, device, fp16=args.fp16)
    outputs: List[str] = []
    bs = args.batch_size
    for i in tqdm(range(0, len(cleaned), bs), total=(len(cleaned)+bs-1)//bs, desc="Detox gate", unit="batch"):
        batch = cleaned[i : i + bs]
        batch_mask = mask[i : i + bs]
        to_edit = [t for t, m in zip(batch, batch_mask) if m]
        if not to_edit:
            outputs.extend(batch)
            continue
        edited = llama.detox(to_edit, batch_size=max(1, bs // 2), max_new_tokens=args.max_new_tokens, temperature=args.temperature, top_p=args.top_p, single_line=True)
        it = iter(edited)
        merged = [next(it) if m else orig for orig, m in zip(batch, batch_mask)]
        outputs.extend(merged)

    # Safety fallback
    final_out = [o if (o and str(o).strip()) else t for o, t in zip(outputs, texts)]

    # Save TSV + optional ZIP
    tsv_path = os.path.join(args.output_dir, "submission_wordrm_llama.tsv")
    pd.DataFrame({"ID": ids, "tat_toxic": texts, "tat_detox1": final_out}).to_csv(tsv_path, sep="\t", index=False)
    logging.info("Saved TSV: %s", tsv_path)
    if args.make_zip:
        zip_path = os.path.join(args.output_dir, "submission_wordrm_llama.zip")
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.write(tsv_path, arcname=os.path.basename(tsv_path))
        logging.info("Saved ZIP: %s", zip_path)


if __name__ == "__main__":
    main()

