import argparse
import csv
import logging
import os
import re
import zipfile
from typing import List, Optional

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
)


# Bind to GPU 1 by default
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "1")


SAFE_REMOVE_TOKENS = [
    # Ñ€ÑƒÑÑÐºÐ¸Ðµ Ð¼Ð°Ñ‚Ð½Ñ‹Ðµ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸, Ñ‡Ð°ÑÑ‚Ð¾ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÑŽÑ‰Ð¸ÐµÑÑ Ð² Ñ‚Ð°Ñ‚Ð°Ñ€ÑÐºÐ¾Ð¹ Ñ€ÐµÑ‡Ð¸
    "Ð±Ð»Ñ", "Ð±Ð»ÑÑ", "Ð±Ð»ÑÑ‚", "Ð±Ð»ÑÑ‚ÑŒ", "Ð±Ð»ÑÐ´ÑŒ", "Ð±Ð»ÑÑ‚", "Ð±Ð»Ñ", "Ð±Ð»Ñ‚",

    "Ð½Ð°Ñ…", "Ð½Ð°Ñ…ÑƒÐ¹", "Ð½Ð°Ñ…Ñ€ÐµÐ½", "Ð½Ð°Ñ…ÐµÑ€", "Ð½Ð°Ñ…ÑƒÑ", "Ð½Ð¸Ñ…ÑƒÑ", "ÐµÐ±Ð°",

    # Ð¿Ð¸Ð·Ð´-ÐºÐ¾Ñ€ÐµÐ½ÑŒ (ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÐ¶Ð´Ð¾Ð¼ÐµÑ‚Ð¸Ñ, Ð±ÐµÐ· ÑÐ¼Ñ‹ÑÐ»Ð°)
    "Ð¿Ð¸Ð·Ð´Ð°", "Ð¿ÐµÐ·Ð´Ð°", "Ð¿ÐµÐ·Ð´Ð¾Ñ", "Ð¿Ð¸Ð·Ð´Ð¾Ñ", "Ð¿Ð¸Ð·Ð´ÑŽÐº", "Ð¿ÐµÐ·Ð´ÑŽÐº",
    "Ð¿Ð¸Ð·Ð´ÐµÑ†", "Ð¿ÐµÐ·Ð´ÑÑ†", "Ð¿ÐµÐ·Ð´ÐµÑ", "Ð¿Ð¸Ð·Ð´Ð¾ÑÐ¸Ðº", "Ð¿Ð¸Ð·Ð´Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ",
    "Ð¿Ð¸Ð·Ð´Ð¾Ð±Ð¾Ð»", "Ð¿Ð¸Ð·Ð´Ð°Ð±Ð¾Ð»",

    # Ñ…ÑƒÐµ-ÐºÐ¾Ñ€ÐµÐ½ÑŒ
    "Ñ…ÑƒÐµÐ¿Ð»ÐµÑ‚", "Ñ…ÑƒÐµÐ¿Ð»Ñ‘Ñ‚", "Ñ…ÑƒÐµÑÐ¾Ñ", "Ñ…ÑƒÐµÑÐ¾ÑÐ»Ð°Ñ€", "Ñ…ÑƒÐµÐ¿Ð°Ñ‡Ð¼Ð°Ðº",
    "Ñ…ÑƒÐµÐ±ÐµÑ‚", "Ñ…ÑƒÐµÐ±Ð¾Ð·", "Ñ…ÑƒÐ¹Ð»Ð¾Ð¿", "Ñ…ÑƒÐ¹Ð»Ð¾", "Ñ…ÑƒÐ¹Ð½Ñ",
    "Ñ…ÑƒÐ¸Ñ‚Ð°", "Ñ…ÑƒÑ‘Ð²", "Ñ…ÑƒÐµÐ²Ð¸Ð½Ð°",

    # Ð¾ÑÐºÐ¾Ñ€Ð±Ð»ÐµÐ½Ð¸Ñ, Ð½Ðµ Ð½ÐµÑÑƒÑ‰Ð¸Ðµ Ñ„Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¼Ñ‹ÑÐ»Ð°
    "Ð¿Ð¸Ð´Ð¾Ñ€", "Ð¿Ð¸Ð´Ð°Ñ€Ð°Ñ", "Ð¿Ð¸Ð´Ð¾Ñ€Ð°Ñ", "Ð¿Ð¸Ð´Ð°Ñ€", "Ð¿Ð¸Ð´Ñ€Ð¸Ð»Ð»Ð°",
    "Ð¿Ð¸Ð´Ð°Ñ€Ð¾Ðº", "Ð¿Ð¸Ð´Ñ€", "Ð¿Ð¸Ð´Ñ€Ð¸Ñ", "Ð¿ÐµÐ´Ð¸Ðº", "Ð³Ð¾Ð¼Ð¸Ðº",

    # Ð¼ÑÐ³ÐºÐ°Ñ Ñ€ÑƒÐ³Ð°Ð½ÑŒ
    "Ð´Ð¾Ð»Ð±Ð°ÐµÐ±", "Ð´Ð¾Ð»Ð±Ð¾ÐµÐ±", "Ð´Ð¾Ð»Ð±Ð¾Ñ‘Ð±", "Ð´ÐµÐ±Ð¸Ð»", "Ð¸Ð´Ð¸Ð¾Ñ‚",
    "Ñ‡Ð¼Ð¾", "Ñ‡Ð¼Ð¾ÑˆÐ½Ð¸Ðº",

    # ÑÐºÐ°Ñ‚Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÑƒÐ³Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°
    "ÑÑ€Ð°Ð½Ð´ÐµÐ»ÑŒ", "ÑÑ€Ð°Ñ‚Ñ‹Ð¹", "Ð³Ð¾Ð²Ð½ÑŽÐº", "Ð³Ð°Ð²Ð½ÑŽÐº",

    # Â«Ð¶Ð¾Ð¿Ð°Â»-Ñ€ÑƒÐ³Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°
    "Ð¶Ð¾Ð¿Ð°", "Ð¶Ð¾Ð¿Ð¾Ñ€Ð¾Ñ‚Ñ‹Ð¹", "Ð¶Ð¾Ð¿Ð¾Ð»Ð¸Ð·Ð°", "Ð¶Ð¾Ð¿Ð°ÑˆÐ½Ð¸Ðº",

    # Ñ‚Ð°Ñ‚Ð°Ñ€ÑÐºÐ¸Ðµ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÐ¶Ð´Ð¾Ð¼ÐµÑ‚Ð¸Ñ
    "Ó™Ñ‚Ñ‚Ó™Ð³ÐµÐ½Ó™", "Ó™Ñ‚Ñ‚Ó™Ð³ÐµÐ½Ó™Ò»Ðµ", "Ó™Ð¿Ó™Ñ‚", "Ó™Ð¿Ó™Ó™Ñ‚",

    # Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ€ÑƒÐ³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚Ñ‹
    "ÑÑƒÐºÐ°", "ÑÑƒÑ‡Ð°Ñ€Ð°", "Ð¼Ñ€Ð°Ð·ÑŒ", "Ñ‚Ð²Ð°Ñ€ÑŒ",

    # Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ñ‹Ðµ ÑÐ¼Ð¾Ð´Ð·Ð¸ Ð¸ Ð¸Ñ… ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ð¸
    "ðŸ˜¡", "ðŸ¤¬", "ðŸ‘¿", "ðŸ˜ ",
    "ðŸ˜", "ðŸ˜’", "ðŸ™„",
    "ðŸ’©", "ðŸ‘", "ðŸ‘‰ðŸ‘Œ", "ðŸ‘ˆðŸ‘‰", "ðŸ”¥ðŸ‘", "ðŸ†", "ðŸ’¦ðŸ‘", "ðŸ’¦ðŸ†",
    "ðŸ¤¡", "ðŸ–•", "ðŸ–•ðŸ»", "ðŸ–•ðŸ½", "ðŸ–•ðŸ¿", "ðŸ¤¦", "ðŸ¤¦â€â™‚ï¸", "ðŸ¤¦â€â™€ï¸",
    "ðŸ¤·", "ðŸ¤·â€â™‚ï¸", "ðŸ¤·â€â™€ï¸",
    "ðŸ˜‚ðŸ’©", "ðŸ’©ðŸ˜‚", "ðŸ¤¡ðŸ˜‚", "ðŸ˜‚ðŸ¤¡", "ðŸ¤¬ðŸ¤¬", "ðŸ˜¡ðŸ¤¬", "ðŸ™ƒ", "ðŸ«„",

    # Ð·Ð°ÐµÐ±-ÐºÐ¾Ñ€ÐµÐ½ÑŒ, ÐºÐ°Ðº Ñ‡Ð¸ÑÑ‚Ñ‹Ð¹ Ð¼Ð°Ñ‚Ð½Ñ‹Ð¹ Ð¼Ð°Ñ€ÐºÐµÑ€
    "Ð·Ð°ÐµÐ±Ð°Ð»", "Ð·Ð°ÐµÐ±Ð°Ð»Ð¸", "Ð·Ð°ÐµÐ±Ð°Ð»Ð¸ÑÑŒ", "Ð·Ð°ÐµÐ±Ð¸ÑÑŒ", "Ð·Ð°Ð¸Ð¿Ð°Ð»Ð¸",

    # ÐµÐ±-ÐºÐ¾Ñ€ÐµÐ½ÑŒ, ÑƒÐ¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº Ð¼Ð°Ñ‚Ð½Ð¾Ðµ Ð¼ÐµÐ¶Ð´Ð¾Ð¼ÐµÑ‚Ð¸Ðµ Ð¸ Ð¶Ñ‘ÑÑ‚ÐºÐ¾Ðµ Ð¾ÑÐºÐ¾Ñ€Ð±Ð»ÐµÐ½Ð¸Ðµ
    "ÐµÐ±Ð°Ñ‚ÑŒ", "ÐµÐ±Ð°Ð½Ñ‹Ð¹", "ÐµÐ±Ð°Ð½Ð°Ñ", "ÐµÐ±Ð°Ð½ÑƒÑ‚Ñ‹Ð¹", "ÐµÐ±Ð°Ð½ÑƒÑ‚Ð°Ñ",
    "ÐµÐ±Ð°Ð½ÑƒÐ»ÑÑ", "ÐµÐ±Ð»Ð°Ð½",

    # Ð¾Ñ…Ñƒ-ÐºÐ¾Ñ€ÐµÐ½ÑŒ Ð² ÑÐ²Ð½Ð¾Ð¹ Ð¼Ð°Ñ‚Ð½Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
    "Ð¾Ñ…ÑƒÐµÐ»", "Ð¾Ñ…ÑƒÐµÑ‚ÑŒ", "Ð¾Ñ…ÑƒÐµÐ½Ð½Ð¾", "Ð¾Ñ…ÑƒÐµÐ½Ð½Ñ‹Ð¹", "Ð¿Ñ€Ð¸Ñ…ÑƒÐµÐ»",

    # ÑÐ¸Ð»ÑŒÐ½Ñ‹Ðµ Ð¾Ð´Ð½Ð¾ÑÐ»Ð¾Ð²Ð½Ñ‹Ðµ Ð¾ÑÐºÐ¾Ñ€Ð±Ð»ÐµÐ½Ð¸Ñ
    "Ð¼ÑƒÐ´Ð°Ðº", "Ð¼ÑƒÐ´Ð°ÐºÐ¸",
    "ÑˆÐ°Ð»Ð°Ð²Ð°", "ÑˆÐ°Ð»Ð°Ð²Ñ‹",
    "ÑÐ²Ð¾Ð»Ð¾Ñ‡ÑŒ", "ÑÐ²Ð¾Ð»Ð¾Ñ‡Ð¸",
    "ÐµÐ±Ð»Ð°Ð½", 
]



def get_device() -> torch.device:
    if torch.cuda.is_available():
        try:
            torch.cuda.set_device(0)
        except Exception:
            pass
        return torch.device("cuda:0")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


class ToxicityClassifier:
    def __init__(self, device: torch.device, max_len: int = 512):
        self.tok = AutoTokenizer.from_pretrained(
            "textdetox/xlmr-large-toxicity-classifier-v2"
        )
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "textdetox/xlmr-large-toxicity-classifier-v2"
        ).to(device)
        self.model.eval()
        self.device = device
        self.max_len = max_len

    @torch.no_grad()
    def score(self, texts: List[str], batch_size: int = 16) -> np.ndarray:
        probs = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            enc = self.tok(
                batch,
                padding=True,
                truncation=True,
                max_length=self.max_len,
                return_tensors="pt",
            ).to(self.device)
            logits = self.model(**enc).logits
            p = torch.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()
            probs.append(p)
        return np.concatenate(probs) if probs else np.array([])


class SDMDetox:
    def __init__(self, device: torch.device, fp16: bool = True, max_new_tokens: int = 128):
        # s-nlp/mt0-xl-detox-sdm-full
        self.tok = AutoTokenizer.from_pretrained("s-nlp/mt0-xl-detox-mpd")
        self.model = AutoModelForSeq2SeqLM.from_pretrained("s-nlp/mt0-xl-detox-mpd").to(device)
        if fp16 and device.type == "cuda":
            try:
                self.model.half()
            except Exception:
                pass
        self.model.eval()
        self.device = device
        self.max_new_tokens = max_new_tokens

    @torch.no_grad()
    def detox(self, texts: List[str], batch_size: int = 8) -> List[str]:
        outputs: List[str] = []
        prompts = [f"Detoxify: {t}" for t in texts]
        total = (len(prompts) + batch_size - 1) // batch_size
        for i in tqdm(range(0, len(prompts), batch_size), total=total, desc="SDM detox", unit="batch"):
            batch = prompts[i : i + batch_size]
            enc = self.tok(batch, return_tensors="pt", padding=True, truncation=True, max_length=512).to(self.device)
            gen = self.model.generate(**enc, max_new_tokens=self.max_new_tokens, do_sample=False)
            outputs.extend(self.tok.batch_decode(gen, skip_special_tokens=True))
        return outputs


# =============================
# Cleaning: word removal + regex
# =============================


def build_regexes() -> List[re.Pattern]:
    patterns = []
    # Obfuscations around "Ð±Ð»ÑÑ‚ÑŒ": Ð±Ð»*Ñ‚ÑŒ, Ð±Ð»##Ñ‚ÑŒ, Ð±Ð»ÐµÐ°Ñ‚ÑŒ, Ð±Ð»ÑÑ‚
    patterns.append(re.compile(r"Ð±\s*Ð»\s*[Ðµe*#]?\s*[Ñya@4]+\s*(?:Ñ‚|Ñ‚ÑŒ|\*+|#+)", re.IGNORECASE))
    # General roots with star/hash in the middle (Ñ…Ñƒ*, Ð¿Ð¸*Ð´*, Ð½Ð°*ÑƒÐ¹ etc.)
    patterns.append(re.compile(r"Ñ…\s*Ñƒ\s*[Ð¹Ð¸i\*#]+[Ð°-ÑÑ‘]*", re.IGNORECASE))
    patterns.append(re.compile(r"Ð¿\s*Ð¸\s*Ð·\s*Ð´[Ð°-ÑÑ‘\*#]+", re.IGNORECASE))
    patterns.append(re.compile(r"Ð½\s*Ð°\s*Ñ…[Ð°-ÑÑ‘\*#]+", re.IGNORECASE))
    return patterns


REGEXES = build_regexes()


def cleanse_text(text: str) -> str:
    t = str(text)
    # Remove explicit tokens first (case-insensitive, as standalone or embedded)
    for tok in SAFE_REMOVE_TOKENS:
        t = re.sub(re.escape(tok), "", t, flags=re.IGNORECASE)
    # Regex-based removals for obfuscations
    for rx in REGEXES:
        t = rx.sub("", t)
    # Collapse spaces
    t = re.sub(r"\s+", " ", t).strip()
    return t


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input_tsv", type=str, default="DETOX_TATAR/data/dev_inputs.tsv")
    ap.add_argument("--output_dir", type=str, default="DETOX_TATAR/outputs_wordrm_sdm")
    ap.add_argument("--threshold", type=float, default=0.5)
    ap.add_argument("--batch_size", type=int, default=16)
    ap.add_argument("--fp16", type=lambda x: str(x).lower() == "true", default=True)
    ap.add_argument("--zip", dest="make_zip", action="store_true")
    args = ap.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
    logging.info("CUDA_VISIBLE_DEVICES=%s", os.environ.get("CUDA_VISIBLE_DEVICES"))
    device = get_device()
    logging.info("Device: %s", device)

    # Load data
    df = pd.read_csv(args.input_tsv, sep="\t", quoting=csv.QUOTE_NONE)
    if not {"ID", "tat_toxic"}.issubset(df.columns):
        raise ValueError("Input TSV must contain 'ID' and 'tat_toxic'")
    ids = df["ID"].tolist()
    texts = df["tat_toxic"].astype(str).tolist()
    logging.info("Loaded %d rows", len(texts))

    # Cleaning
    logging.info("Applying word removal + regex cleaning")
    cleaned = [cleanse_text(t) for t in tqdm(texts, desc="Clean", unit="row")]

    # Classifier gating
    clf = ToxicityClassifier(device)
    logging.info("Scoring toxicity after cleaning (gating)")
    probs = clf.score(cleaned, batch_size=args.batch_size)
    mask = (probs >= args.threshold).tolist()

    # Detox only masked examples using SDM
    sdm = SDMDetox(device, fp16=args.fp16)
    outputs: List[str] = []
    batch_size = args.batch_size
    for i in tqdm(range(0, len(cleaned), batch_size), total=(len(cleaned)+batch_size-1)//batch_size, desc="Detox gate", unit="batch"):
        batch = cleaned[i : i + batch_size]
        batch_mask = mask[i : i + batch_size]
        to_edit = [t for t, m in zip(batch, batch_mask) if m]
        if not to_edit:
            outputs.extend(batch)
            continue
        edited = sdm.detox(to_edit, batch_size=max(1, batch_size // 2))
        it = iter(edited)
        merged = [next(it) if m else orig for orig, m in zip(batch, batch_mask)]
        outputs.extend(merged)

    # Safety: fallback to original if any empty
    final_out = [o if (o and str(o).strip()) else t for o, t in zip(outputs, texts)]

    # Save submission TSV (only required columns) and optional ZIP
    tsv_path = os.path.join(args.output_dir, "submission_wordrm_sdm.tsv")
    pd.DataFrame({"ID": ids, "tat_toxic": texts, "tat_detox1": final_out}).to_csv(tsv_path, sep="\t", index=False)
    logging.info("Saved TSV: %s", tsv_path)
    if args.make_zip:
        zip_path = os.path.join(args.output_dir, "submission_wordrm_sdm.zip")
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.write(tsv_path, arcname=os.path.basename(tsv_path))
        logging.info("Saved ZIP: %s", zip_path)


if __name__ == "__main__":
    main()
